{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/home/himesh/TagCoder/pythonNotebook'\n",
    "#basePath = r'C:\\Users\\Himesh\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '/Positive'\n",
    "negativePathSuffix = '/Negative'\n",
    "tokenizerInPath = basePath + '/tokenizerIn'\n",
    "tokenizerOutPath = basePath + '/tokenizerOut'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smellList = ['ComplexMethod']\n",
    "final_text = \"\"\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\") as read_file:\n",
    "            try:\n",
    "                text = read_file.read()\n",
    "                tokenized_text = tokenizer.tokenize(text,padding = \"max_length\")\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                modint = (len(input_ids)) % 512\n",
    "                print(modint)\n",
    "                length = len(input_ids) - modint\n",
    "            \n",
    "                input_ids = input_ids[0:length]\n",
    "                final_text += ' '.join(map(str, input_ids))+'\\n\\n'\n",
    "            except Exception as e:\n",
    "                print('exception')\n",
    "                pass\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "        #out_file.touch(exist_ok=True)\n",
    "        #print(final_text)\n",
    "        out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\") as read_file:\n",
    "        try:\n",
    "            text = read_file.read()\n",
    "            tokenized_text = tokenizer.tokenize(text,padding = \"max_length\")\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            modint = (len(input_ids)) % 512\n",
    "            print(modint)\n",
    "            length = len(input_ids) - modint\n",
    "           \n",
    "            input_ids = input_ids[0:length]\n",
    "            final_text += ' '.join(map(str, input_ids))+'\\n\\n'\n",
    "        except Exception as e:\n",
    "            print('exception')\n",
    "            pass\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #out_file.touch(exist_ok=True)\n",
    "    #print(final_text)\n",
    "    out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posInput = []\n",
    "num_lines_pos = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\"))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    posInput = np.fromstring(text, sep=\" \").tolist()\n",
    "    print(len(posInput))\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     posInput.append(arr)\n",
    "\n",
    "negInput = []\n",
    "num_lines_neg = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\"))\n",
    "\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    negInput = np.fromstring(text, dtype=np.int32, sep=\" \").tolist()\n",
    "\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     negInput.append(arr)\n",
    "\n",
    "num_lines_all =  num_lines_pos if num_lines_pos < num_lines_neg else num_lines_pos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posInputLen = len(posInput)\n",
    "negInputLen = len(negInput)\n",
    "print(str(posInputLen)+\"  \"+str(negInputLen))\n",
    "train_data = []\n",
    "test_data = []\n",
    "posSize = ceil(posInputLen*train_ratio) - ceil(posInputLen*train_ratio) % 512\n",
    "print(posSize)\n",
    "negSize = ceil(negInputLen*train_ratio) - ceil(negInputLen*train_ratio) % 512\n",
    "\n",
    "test_data.append(posInput[posSize+1:])\n",
    "test_data.append(negInput[negSize+1:])\n",
    "\n",
    "test_label = np.empty(shape=[len(test_data)], dtype=np.float32)\n",
    "test_label[0:posSize] = 1.0\n",
    "test_label[posSize+1:] = 0.0\n",
    "\n",
    "train_data.append(posInput[0:posSize])\n",
    "train_data.append(negInput[0:negSize])\n",
    "train_data_flattened = list(chain.from_iterable(train_data))\n",
    "shuffle(train_data_flattened)\n",
    "#print(train_data[1])\n",
    "train_data_np = np.array(train_data_flattened)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np = train_data_np.reshape(-1,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_lstm(train_data, smell, layers=1, encoding_dimension=8, no_of_epochs=10, with_bottleneck=True, is_final=False):\n",
    "    \n",
    "    encoding_dim = encoding_dimension\n",
    "    input_layer = Input(shape=(512, 1))\n",
    "    # input_layer = BatchNormalization()(input_layer)\n",
    "    no_of_layers = layers\n",
    "    prev_layer = input_layer\n",
    "\n",
    "    for i in range(no_of_layers):\n",
    "        encoder = LSTM(int(encoding_dim / pow(2, i)),\n",
    "                       # activation=\"relu\",\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = encoder \n",
    "    \n",
    "    if with_bottleneck:\n",
    "        prev_layer = LSTM(int(encoding_dim / pow(2, no_of_layers + 1)),\n",
    "                          # activation=\"relu\",\n",
    "                          return_sequences=True,\n",
    "                          recurrent_dropout=0.1,\n",
    "                          dropout=0.1)(prev_layer)\n",
    "    for j in range(no_of_layers - 1, -1, -1):\n",
    "        decoder = LSTM(int(encoding_dim / pow(2, j)),\n",
    "                       # activation='relu',\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = decoder\n",
    "    prev_layer = TimeDistributed(Dense(1))(prev_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=prev_layer)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "    autoencoder.summary()   \n",
    "\n",
    "\n",
    "    batch_sizes = [32, 64]\n",
    "    b_size = int(len(train_data) / 512)\n",
    "    if b_size > len(batch_sizes) - 1:\n",
    "        b_size = len(batch_sizes) - 1\n",
    "    history = autoencoder.fit(train_data,\n",
    "                              train_data,\n",
    "                              epochs=no_of_epochs,\n",
    "                              batch_size=batch_sizes[b_size],\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2,\n",
    "                              shuffle=True).history\n",
    "\n",
    "    predictions = autoencoder.predict(test_data)\n",
    "    predictions = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    test_data = test_data.reshape(test_data.eval_data.shape[0], test_data.eval_data.shape[1])\n",
    "    mse = np.mean(np.power(test_data - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                             'True_class': test_label})\n",
    "    print(error_df.describe())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1, 2]\n",
    "encoding_dim = [8, 16, 32]\n",
    "epochs = 10\n",
    "\n",
    "for layer in layers:\n",
    "        for bottleneck in [True]:\n",
    "            for encoding in encoding_dim:\n",
    "                max_f1 = autoencoder_lstm(train_data, smell, layers=layer,encoding_dimension=encoding,no_of_epochs=epochs, with_bottleneck=bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
