{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basePath = '/home/himesh/TagCoder/pythonNotebook'\n",
    "basePath = r'C:\\Users\\Himesh\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '/Positive'\n",
    "negativePathSuffix = '/Negative'\n",
    "tokenizerInPath = basePath + '\\\\tokenizerIn'\n",
    "tokenizerOutPath = basePath + '\\\\tokenizerOut'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed786dcc0154b5cb07265d3f1d02320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himesh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Himesh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704aade47fa147fd94020317edae6c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 13) (2576643067.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open(os.path.join(smellPath, file),\"r, encoding=\"utf8\" as read_file:\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 13)\n"
     ]
    }
   ],
   "source": [
    "tokenized_methods = []\n",
    "y_pos = []\n",
    "\n",
    "smellList = ['ComplexMethod']\n",
    "final_text = \"\"\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        #print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\",encoding=\"utf8\") as read_file:\n",
    "           \n",
    "            text = read_file.read()\n",
    "            tokenized_method = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "            tokenized_methods.append(tokenized_method)\n",
    "            y_pos.append(1)\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    # with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #     #out_file.touch(exist_ok=True)\n",
    "    #     #print(final_text)\n",
    "    #     out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\",encoding=\"utf8\") as read_file:\n",
    "        text = read_file.read()\n",
    "        tokenized_method = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "        tokenized_methods.append(tokenized_method)\n",
    "        y_pos.append(0)\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "# with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "#     #out_file.touch(exist_ok=True)\n",
    "#     #print(final_text)\n",
    "#     out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_methods = []\n",
    "\n",
    "for tokenized_method in tokenized_methods:\n",
    "    encoded_method = model(**tokenized_method).last_hidden_state\n",
    "    encoded_methods.append(encoded_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_methods, y_pos, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = encoded_methods[0].shape[1:]\n",
    "\n",
    "# Define the input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Define the LSTM layers\n",
    "lstm1 = LSTM(64, return_sequences=True)(inputs)\n",
    "lstm2 = LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = LSTM(64, return_sequences=True)(lstm2)\n",
    "\n",
    "# Define the output layer\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'))(lstm3)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posInput = []\n",
    "num_lines_pos = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\"))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    posInput = np.fromstring(text, sep=\" \").tolist()\n",
    "    print(len(posInput))\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     posInput.append(arr)\n",
    "\n",
    "negInput = []\n",
    "num_lines_neg = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\"))\n",
    "\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    negInput = np.fromstring(text, dtype=np.int32, sep=\" \").tolist()\n",
    "\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     negInput.append(arr)\n",
    "\n",
    "num_lines_all =  num_lines_pos if num_lines_pos < num_lines_neg else num_lines_pos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posInputLen = len(posInput)\n",
    "negInputLen = len(negInput)\n",
    "print(str(posInputLen)+\"  \"+str(negInputLen))\n",
    "train_data = []\n",
    "test_data = []\n",
    "posSize = ceil(posInputLen*train_ratio) - ceil(posInputLen*train_ratio) % 512\n",
    "print(posSize)\n",
    "negSize = ceil(negInputLen*train_ratio) - ceil(negInputLen*train_ratio) % 512\n",
    "\n",
    "test_data.append(posInput[posSize+1:])\n",
    "test_data[0] = test_data[0][0:len(test_data[0]) - (len(test_data[0])%512)]\n",
    "test_data.append(negInput[negSize+1:])\n",
    "test_data[1] = test_data[1][0:len(test_data[1]) - (len(test_data[1])%512)]\n",
    "\n",
    "test_data_flattened = list(chain.from_iterable(test_data))\n",
    "test_data_np = np.array(test_data_flattened)\n",
    "\n",
    "test_label = np.empty(shape=[len(test_data_np)], dtype=np.float32)\n",
    "print(len(test_label))\n",
    "test_label[0:posSize] = 1.0\n",
    "test_label[posSize+1:] = 0.0\n",
    "\n",
    "total_train_data = 0\n",
    "train_data.append(posInput[0:posSize])\n",
    "train_data[0] = train_data[0][0:len(train_data[0]) - (len(train_data[0])%512)]\n",
    "total_train_data += len(train_data[0])\n",
    "train_data.append(negInput[0:negSize])\n",
    "train_data[1] = train_data[1][0:len(train_data[1]) - (len(train_data[1])%512)]\n",
    "total_train_data += len(train_data[1])\n",
    "print('total_train_data'+str(total_train_data))\n",
    "train_data_flattened = list(chain.from_iterable(train_data))\n",
    "#shuffle(train_data_flattened)\n",
    "#print(train_data[1])\n",
    "train_data_np = np.array(train_data_flattened)\n",
    "train_data_np = train_data_np.reshape(-1,512,1)\n",
    "shuffle(train_data_np)\n",
    "print('arr shape')\n",
    "print(train_data_np.shape)\n",
    "test_data_np = test_data_np.reshape(len(test_label),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_lstm(train_data, test_data_np, smell, layers=1, encoding_dimension=8, no_of_epochs=10, with_bottleneck=True, is_final=False):\n",
    "    \n",
    "    encoding_dim = encoding_dimension\n",
    "    input_layer = Input(shape=(512, 1))\n",
    "    # input_layer = BatchNormalization()(input_layer)\n",
    "    no_of_layers = layers\n",
    "    prev_layer = input_layer\n",
    "    \n",
    "    for i in range(no_of_layers):\n",
    "        encoder = LSTM(int(encoding_dim / pow(2, i)),\n",
    "                        #activation=\"relu\",\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = encoder \n",
    "    \n",
    "    if with_bottleneck:\n",
    "        prev_layer = LSTM(int(encoding_dim / pow(2, no_of_layers + 1)),\n",
    "                         #activation=\"relu\",\n",
    "                          return_sequences=True,\n",
    "                          recurrent_dropout=0.1,\n",
    "                          dropout=0.1)(prev_layer)\n",
    "    for j in range(no_of_layers - 1, -1, -1):\n",
    "        decoder = LSTM(int(encoding_dim / pow(2, j)),\n",
    "                        #activation='relu',\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = decoder\n",
    "    prev_layer = TimeDistributed(Dense(1))(prev_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=prev_layer)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "    autoencoder.summary()   \n",
    "\n",
    "\n",
    "    batch_sizes = [32, 64]\n",
    "    b_size = int(len(train_data) / 512)\n",
    "    if b_size > len(batch_sizes) - 1:\n",
    "        b_size = len(batch_sizes) - 1\n",
    "    history = autoencoder.fit(train_data,\n",
    "                              train_data,\n",
    "                              epochs=no_of_epochs,\n",
    "                              batch_size=batch_sizes[b_size],\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2,\n",
    "                              shuffle=True).history\n",
    "    \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    predictions = autoencoder.predict(test_data_np)\n",
    "    predictions = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    test_data_np = test_data_np.reshape(test_data_np.shape[0], test_data_np.shape[1])\n",
    "    mse = np.mean(np.power(test_data_np - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                             'True_class': test_label})\n",
    "    print(error_df.describe())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,2]\n",
    "encoding_dim = [8, 16, 32]\n",
    "epochs = 100\n",
    "cur_iter = 1\n",
    "skip_iter = 2\n",
    "for layer in layers:\n",
    "        for bottleneck in [True]:\n",
    "            for encoding in encoding_dim:\n",
    "                if cur_iter <= skip_iter:\n",
    "                    cur_iter += 1\n",
    "                    continue\n",
    "                cur_iter += 1\n",
    "                autoencoder_lstm(train_data_np,test_data_np, smell, layers=layer,encoding_dimension=encoding,no_of_epochs=epochs, with_bottleneck=bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
